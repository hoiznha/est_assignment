#step2_embedding.py

"""
BGE-m3-ko ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± ë° ChromaDB ì €ì¥
- ëª¨ë¸: dragonkue/BGE-m3-ko (ì˜¤í”ˆì†ŒìŠ¤)
- ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥ (API í‚¤ ë¶ˆí•„ìš”)
"""

import os
import json
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import torch  # ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ í•„ìš”

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

# =============================================================================
# BGE-m3-ko ì„ë² ë”© í´ë˜ìŠ¤
# =============================================================================

class BGEEmbedding:
    """
    BGE-m3-ko ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ í•¸ë“¤ëŸ¬
    HuggingFace Transformers ê¸°ë°˜
    """

    def __init__(self, model_name: str = "dragonkue/BGE-m3-ko", device: Optional[str] = None):
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError:
            raise ImportError(
                "âŒ sentence-transformersë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:\n"
                "pip install sentence-transformers"
            )
        
        import torch
        
        self.model_name = model_name
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA, MPS, CPU)
        if device is None:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        # CPU í™˜ê²½ ìµœì í™” ì„¤ì •
        if self.device == "cpu":
            # CPU ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™” (Railway í™˜ê²½ ê³ ë ¤)
            num_threads = min(4, torch.get_num_threads())  # ìµœëŒ€ 4ìŠ¤ë ˆë“œ
            torch.set_num_threads(num_threads)
            logging.info(f"   CPU ìŠ¤ë ˆë“œ ìˆ˜: {num_threads}")
        
        logging.info(f"ğŸ”„ BGE-m3-ko ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
        logging.info("   â³ ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        # ëª¨ë¸ ë¡œë“œ ìµœì í™” ì˜µì…˜
        model_kwargs = {}
        if self.device == "cuda":
            # CUDA í™˜ê²½: FP16 ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
            try:
                model_kwargs = {'torch_dtype': torch.float16}
                logging.info("   ìµœì í™”: FP16 ì‚¬ìš© (CUDA)")
            except:
                pass
        elif self.device == "cpu":
            # CPU í™˜ê²½: ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  ìµœì í™”
            model_kwargs = {}
            logging.info("   ìµœì í™”: CPU ì¶”ë¡  ìµœì í™”")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer(
            model_name, 
            device=self.device,
            model_kwargs=model_kwargs if model_kwargs else None
        )
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (ë“œë¡­ì•„ì›ƒ ë“± ë¹„í™œì„±í™”)
        self.model.eval()
        
        # CPU í™˜ê²½ì—ì„œ ì¶”ê°€ ìµœì í™”
        if self.device == "cpu":
            # torch.jit.scriptë¡œ ìµœì í™” ì‹œë„ (ì„ íƒì )
            try:
                # ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ìµœì í™”í•  ìˆ˜ ìˆì§€ë§Œ, sentence-transformersëŠ” ì´ë¯¸ ìµœì í™”ë˜ì–´ ìˆìŒ
                pass
            except:
                pass
        
        logging.info(f"âœ… BGE-m3-ko ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        logging.info(f"   ëª¨ë¸: {model_name}")
        logging.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        logging.info(f"   ìµœì í™”: í™œì„±í™”ë¨")

    def embed_documents(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        ë¬¸ì„œ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            logging.warning("âš ï¸ ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        logging.info(f"ğŸ“Š {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # BGE ëª¨ë¸ì€ retrievalì„ ìœ„í•´ "passage: " í”„ë¦¬í”½ìŠ¤ ì‚¬ìš©
        passages = [f"passage: {text}" for text in texts]
        
        # ë°°ì¹˜ ì„ë² ë”© (ìµœì í™” ì˜µì…˜ ì ìš©)
        # CPU í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
            embeddings = self.model.encode(
                passages,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
            )
        
        # numpy arrayë¥¼ listë¡œ ë³€í™˜
        embeddings_list = [emb.tolist() for emb in embeddings]
        
        logging.info(f"âœ… {len(embeddings_list)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        logging.info(f"ğŸ“ ë²¡í„° ì°¨ì›: {len(embeddings_list[0]) if embeddings_list else 0}")
        
        return embeddings_list

    def embed_query(self, query: str) -> Optional[List[float]]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”©
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        if not query or not query.strip():
            logging.warning("âš ï¸ ë¹ˆ ì¿¼ë¦¬ì…ë‹ˆë‹¤.")
            return None
        
        # BGE ëª¨ë¸ì€ ê²€ìƒ‰ ì¿¼ë¦¬ì— "query: " í”„ë¦¬í”½ìŠ¤ ì‚¬ìš©
        query_with_prefix = f"query: {query}"
        
        # ë‹¨ì¼ ì„ë² ë”© (ìµœì í™” ì˜µì…˜ ì ìš©)
        # CPU í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ ë° ì†ë„ í–¥ìƒ
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
            embedding = self.model.encode(
                query_with_prefix,
                convert_to_numpy=True,
                normalize_embeddings=True,
                batch_size=1  # ë‹¨ì¼ ì¿¼ë¦¬ ìµœì í™”
            )
        
        return embedding.tolist()

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.model.get_sentence_embedding_dimension()


# =============================================================================
# ChromaDB í•¸ë“¤ëŸ¬
# =============================================================================

class ChromaVectorDB:
    """ChromaDB ì˜êµ¬ ë²¡í„° ì €ì¥ì†Œ"""

    def __init__(
        self, 
        collection_name: str = "perso_qa_collection", 
        persist_dir: str = "/chroma_db",
        recreate: bool = False
    ):
        try:
            import chromadb
        except ImportError:
            raise ImportError("âŒ chromadbë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install chromadb")

        self.persist_dir = persist_dir
        self.collection_name = collection_name
        
        os.makedirs(persist_dir, exist_ok=True)
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=persist_dir)

        # ì»¬ë ‰ì…˜ ì¬ìƒì„±
        if recreate:
            try:
                self.client.delete_collection(collection_name)
                logging.info(f"ğŸ§¹ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ ì™„ë£Œ")
            except Exception:
                pass

        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        )
        
        logging.info(f"âœ… ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
        logging.info(f"   ì»¬ë ‰ì…˜: {collection_name}")
        logging.info(f"   ì €ì¥ ìœ„ì¹˜: {persist_dir}")
        logging.info(f"   ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {self.collection.count()}")

    def add_documents(
        self, 
        ids: List[str], 
        embeddings: List[List[float]], 
        documents: List[str], 
        metadatas: List[Dict]
    ):
        """ë¬¸ì„œ ì¶”ê°€"""
        
        # ì…ë ¥ ê²€ì¦
        if not (len(ids) == len(embeddings) == len(documents) == len(metadatas)):
            raise ValueError(
                f"âŒ ì…ë ¥ ê¸¸ì´ ë¶ˆì¼ì¹˜: ids={len(ids)}, embeddings={len(embeddings)}, "
                f"documents={len(documents)}, metadatas={len(metadatas)}"
            )
        
        if not embeddings:
            raise ValueError("âŒ ì„ë² ë”©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        logging.info(f"ğŸ’¾ {len(ids)}ê°œ ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥ ì¤‘...")
        
        try:
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
            )
            logging.info(f"âœ… {len(ids)}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 3,
        filter_metadata: Optional[Dict] = None
    ) -> Dict:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        
        if not query_embedding:
            logging.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¿¼ë¦¬ ì„ë² ë”©")
            return {
                'ids': [[]],
                'distances': [[]],
                'documents': [[]],
                'metadatas': [[]],
                'similarities': [[]]
            }
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = self.collection.query(
            query_embeddings=[query_embedding], 
            n_results=top_k,
            where=filter_metadata  # ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ì„ íƒì )
        )
        
        # ìœ ì‚¬ë„ ê³„ì‚° (ê±°ë¦¬ -> ìœ ì‚¬ë„ ë³€í™˜)
        # ChromaDBì˜ ì½”ì‚¬ì¸ ê±°ë¦¬: distance = 1 - cosine_similarity
        # ë”°ë¼ì„œ similarity = 1 - distance
        similarities = [[1 - d for d in results["distances"][0]]]
        results["similarities"] = similarities
        
        return results

    def count(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜"""
        return self.collection.count()


# =============================================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =============================================================================

def create_embeddings_and_store(
    qa_json_path: str,
    chroma_dir: str = "./chroma_db",
    collection_name: str = "perso_qa_collection",
    model_name: str = "dragonkue/BGE-m3-ko",
    recreate: bool = True
) -> Tuple[BGEEmbedding, ChromaVectorDB]:
    """
    BGE-m3-ko ì„ë² ë”© ìƒì„± ë° ChromaDB ì €ì¥ íŒŒì´í”„ë¼ì¸
    
    Args:
        qa_json_path: Q&A JSON íŒŒì¼ ê²½ë¡œ
        chroma_dir: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        recreate: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì¬ìƒì„± ì—¬ë¶€
        
    Returns:
        (BGEEmbedding, ChromaVectorDB) íŠœí”Œ
    """
    
    logging.info("="*60)
    logging.info("ğŸš€ BGE-m3-ko ì„ë² ë”© ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logging.info("="*60)
    
    # 1. BGE ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedder = BGEEmbedding(model_name)
    
    # 2. ChromaDB ì´ˆê¸°í™”
    vector_db = ChromaVectorDB(collection_name, chroma_dir, recreate=recreate)

    # 3. Q&A ë°ì´í„° ë¡œë“œ
    logging.info(f"\nğŸ“‚ Q&A ë°ì´í„° ë¡œë“œ ì¤‘: {qa_json_path}")
    
    if not os.path.exists(qa_json_path):
        raise FileNotFoundError(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {qa_json_path}")
    
    with open(qa_json_path, "r", encoding="utf-8") as f:
        qa_data = json.load(f)
    
    logging.info(f"âœ… {len(qa_data)}ê°œ Q&A ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

    # 4. ë°ì´í„° ì¤€ë¹„
    texts, ids, docs, metas = [], [], [], []
    
    for qa in qa_data:
        question = qa.get('question', '')
        answer = qa.get('answer', '')
        
        # ì§ˆë¬¸ + ë‹µë³€ ê²°í•© (ë” ë‚˜ì€ ê²€ìƒ‰ ì„±ëŠ¥)
        text = f"ì§ˆë¬¸: {question}\në‹µë³€: {answer}"
        
        texts.append(text)
        ids.append(qa.get('id', f"qa_{len(ids):03d}"))
        docs.append(answer)  # ì‹¤ì œ ë‹µë³€ë§Œ ì €ì¥
        
        metadata = qa.get('metadata', {})
        metas.append({
            "question": str(question),
            "category": str(qa.get("category", "")),
            "keywords": ", ".join(metadata.get("keywords", [])),
            "answer_length": int(metadata.get("answer_length", 0))
        })
    
    logging.info(f"ğŸ“ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

    # 5. ì„ë² ë”© ìƒì„±
    logging.info("\nğŸ”„ BGE-m3-koë¡œ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = embedder.embed_documents(texts, batch_size=32)
    
    if len(embeddings) != len(texts):
        logging.warning(
            f"âš ï¸ ì„ë² ë”© ê°œìˆ˜ ë¶ˆì¼ì¹˜: í…ìŠ¤íŠ¸={len(texts)}, ì„ë² ë”©={len(embeddings)}"
        )
    
    # 6. ChromaDBì— ì €ì¥
    logging.info("\nğŸ’¾ ChromaDBì— ì €ì¥ ì¤‘...")
    vector_db.add_documents(ids, embeddings, docs, metas)
    
    # 7. ìµœì¢… í†µê³„
    logging.info("\n" + "="*60)
    logging.info("ğŸ“Š ìµœì¢… í†µê³„")
    logging.info("="*60)
    logging.info(f"ì´ ë¬¸ì„œ ìˆ˜: {vector_db.count()}")
    logging.info(f"ë²¡í„° ì°¨ì›: {embedder.get_embedding_dimension()}")
    logging.info(f"ëª¨ë¸: {model_name}")
    logging.info(f"ì €ì¥ ìœ„ì¹˜: {chroma_dir}")
    logging.info(f"ì»¬ë ‰ì…˜: {collection_name}")
    logging.info("="*60)

    return embedder, vector_db


# =============================================================================
# ê²€ìƒ‰ í•¨ìˆ˜
# =============================================================================

def search_qa(
    query: str, 
    embedder: BGEEmbedding, 
    vector_db: ChromaVectorDB, 
    top_k: int = 3
) -> List[Dict]:
    """
    Q&A ê²€ìƒ‰
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        embedder: BGEEmbedding ì¸ìŠ¤í„´ìŠ¤
        vector_db: ChromaVectorDB ì¸ìŠ¤í„´ìŠ¤
        top_k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_emb = embedder.embed_query(query)
    
    if not query_emb:
        logging.error("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
        return []
    
    # ê²€ìƒ‰ ì‹¤í–‰ 
    results = vector_db.search(query_emb, top_k)
    
    # ê²°ê³¼ í¬ë§·íŒ…
    formatted = []
    for doc, meta, sim, doc_id in zip(
        results["documents"][0], 
        results["metadatas"][0], 
        results["similarities"][0],
        results["ids"][0]
    ):
        formatted.append({
            "id": doc_id,
            "question": meta.get("question"),
            "answer": doc,
            "similarity": sim,
            "category": meta.get("category"),
            "keywords": meta.get("keywords", "")
        })
    
    return formatted


# =============================================================================
# main()
# =============================================================================

def get_project_root() -> Path:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë°˜í™˜"""
    # ì´ íŒŒì¼ì´ backend/ í´ë”ì— ìˆìœ¼ë¯€ë¡œ, parent.parentê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸
    return Path(__file__).resolve().parent.parent.parent.parent


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    project_root = get_project_root()
    backend_dir = project_root / "backend"
    data_dir = backend_dir / "data" / "processed"
    chroma_dir_path = backend_dir / "chroma_db"
    
    # ì„¤ì •
    qa_json_path = str(data_dir / "qa_preprocessed.json")
    chroma_dir = str(chroma_dir_path)
    collection_name = "perso_qa_collection"
    model_name = "dragonkue/BGE-m3-ko"
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(qa_json_path):
        logging.error(f"âŒ Q&A íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {qa_json_path}")
        logging.error(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
        logging.error(f"   ì˜ˆìƒ ìœ„ì¹˜: {qa_json_path}")
        logging.info("ğŸ’¡ ë¨¼ì € step1_preprocess.pyë¥¼ ì‹¤í–‰í•˜ì—¬ qa_preprocessed.json íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ì„ë² ë”© ìƒì„± ë° ì €ì¥
    try:
        embedder, vector_db = create_embeddings_and_store(
            qa_json_path=qa_json_path,
            chroma_dir=chroma_dir,
            collection_name=collection_name,
            model_name=model_name,
            recreate=True
        )
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        logging.info("\n" + "="*60)
        logging.info("ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
        logging.info("="*60)
        
        test_queries = [
            "Perso.aiëŠ” ì–´ë–¤ ì„œë¹„ìŠ¤ì¸ê°€ìš”?"
            # "íšŒì›ê°€ì…ì´ í•„ìš”í•œê°€ìš”?",
            # "ì–´ë–¤ ì–¸ì–´ë¥¼ ì§€ì›í•˜ë‚˜ìš”?"
            # "ìœ íŠœë²„ë“¤ì´ ì´ ì„œë¹„ìŠ¤ë¥¼ ì“°ëŠ” ì´ìœ ê°€ ë¬´ì—‡ì´ë©°, ì–´ë–¤ ê¸°ìˆ ì„ í™œìš©í•˜ë‚˜ìš”?"
        ]
        
        for query in test_queries:
            logging.info(f"\nì§ˆë¬¸: '{query}'")
            logging.info("-"*60)
            
            results = search_qa(query, embedder, vector_db, top_k=3)
            
            for i, r in enumerate(results, 1):
                print(f"\n[{i}] ìœ ì‚¬ë„: {r['similarity']:.4f} | ID: {r['id']}")
                print(f"    ì§ˆë¬¸: {r['question']}")
                print(f"    ë‹µë³€: {r['answer'][:80]}...")
                print(f"    ì¹´í…Œê³ ë¦¬: {r['category']}")
        
        logging.info("\n" + "="*60)
        logging.info("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        logging.info("="*60)
        
    except Exception as e:
        logging.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()